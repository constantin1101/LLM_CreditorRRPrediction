{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''If executed in Google Colab, uncomment the following lines'''\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "#import os\n",
    "#os.chdir('/content/drive/MyDrive/LLM_CreditorRRPrediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = pd.read_csv('transcripts/transcripts.csv', delimiter='|')\n",
    "qna =  pd.read_csv('transcripts/QnA.csv', delimiter='|')\n",
    "\n",
    "# Merge the two dataframes\n",
    "df = pd.merge(transcript, qna[['transcript','filename']], on='filename')\n",
    "\n",
    "# rename transcript_x to presentation and transcript_y to QnA\n",
    "df.rename(columns = {'transcript_x':'presentation', 'transcript_y':'QnA'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = pd.read_csv('data/mapping.csv')\n",
    "\n",
    "# create a new column 'AllNames' that concatenates all versions of 'Company' for a 'CompanyName'\n",
    "mapping['AllNames'] = mapping.groupby('RR_CompanyName')['Transcript_Mapping'].transform(lambda x: ', '.join(x))\n",
    "mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load recovery rates\n",
    "rr = pd.read_csv('data/RR_Bonds.csv')\n",
    "rr = rr[['Ddate', 'RR', 'CompanyName', 'CUSIP', 'LTDIssuance2', 'Intangibility', 'Receivables1']]\n",
    "\n",
    "preprocessed_df = pd.read_csv('data/preprocessed_bond_data.csv')\n",
    "\n",
    "# Add rr columns to preprocessed_df on index\n",
    "preprocessed_df['RR'] = rr['RR']\n",
    "preprocessed_df['Ddate'] = rr['Ddate']\n",
    "preprocessed_df['CompanyName'] = rr['CompanyName']\n",
    "preprocessed_df['CUSIP'] = rr['CUSIP']\n",
    "preprocessed_df['LTDIssuance2'] = rr['LTDIssuance2']\n",
    "preprocessed_df['Intangibility'] = rr['Intangibility']\n",
    "preprocessed_df['Receivables1'] = rr['Receivables1']\n",
    "\n",
    "rr = preprocessed_df\n",
    "\n",
    "# Convert 'Date' column to datetime\n",
    "rr['Ddate'] = pd.to_datetime(rr['Ddate'], errors='coerce')\n",
    "rr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge rr with mapping on CompanyName and RR_CompanyName\n",
    "rr = rr.merge(mapping, left_on='CompanyName', right_on='RR_CompanyName')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Get last earnings call before default'''\n",
    "\n",
    "# join with df on Company and Transcripts_Mapping\n",
    "merged_df = rr.merge(df, left_on='Transcript_Mapping', right_on='Company')\n",
    "print(merged_df['CompanyName'].value_counts())\n",
    "\n",
    "# Ensure the columns are in datetime format\n",
    "merged_df['Date'] = pd.to_datetime(merged_df['Date'])\n",
    "merged_df['Ddate'] = pd.to_datetime(merged_df['Ddate'])\n",
    "\n",
    "merged_df['t_delta'] = merged_df['Ddate'] - merged_df['Date']\n",
    "\n",
    "# Filter out rows where the Date is greater than the Ddate\n",
    "merged_df = merged_df[merged_df['Ddate']>=merged_df['Date']]\n",
    "# Get the last row for each CUSIP\n",
    "merged_df = merged_df.sort_values(by='Date').groupby(['CUSIP']).tail(1)\n",
    "\n",
    "print(merged_df['CompanyName'].value_counts())\n",
    "\n",
    "merged_df.reset_index(drop=True, inplace=True)\n",
    "# Create an ID based on unique CompanyName and Date\n",
    "merged_df['call_ID'] = merged_df.groupby(['Date','CompanyName']).ngroup()\n",
    "\n",
    "print(merged_df['call_ID'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the merged_df to a csv file\n",
    "merged_df.to_csv('transcripts/credit_df.csv', index=False, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df = merged_df[['call_ID', 'presentation', 'QnA', 'CompanyName', 'Ddate']].drop_duplicates().sort_values('call_ID')\n",
    "aggregated_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# export the aggregated_df to a csv file\n",
    "aggregated_df.to_csv('transcripts/aggregated_credit_df.csv', index=False, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Get first earnings call after default'''\n",
    "'''Within first 30 days'''\n",
    "\n",
    "# join with df on Company and Transcripts_Mapping\n",
    "merged_df = rr.merge(df, left_on='Transcript_Mapping', right_on='Company')\n",
    "print(merged_df['CompanyName'].value_counts())\n",
    "\n",
    "# Ensure the columns are in datetime format\n",
    "merged_df['Date'] = pd.to_datetime(merged_df['Date'])\n",
    "merged_df['Ddate'] = pd.to_datetime(merged_df['Ddate'])\n",
    "\n",
    "merged_df['t_delta'] = merged_df['Date'] - merged_df['Ddate']\n",
    "\n",
    "# Filter out rows where the Date is greater than the Ddate\n",
    "merged_df = merged_df[merged_df['Ddate']<merged_df['Date']]\n",
    "merged_df = merged_df[merged_df['t_delta'] <= pd.Timedelta(days=30)]\n",
    "# Get the last row for each CUSIP\n",
    "merged_df = merged_df.sort_values(by='Date').groupby(['CUSIP']).head(1)\n",
    "\n",
    "print(merged_df['CompanyName'].value_counts())\n",
    "\n",
    "merged_df.reset_index(drop=True, inplace=True)\n",
    "# Create an ID based on unique CompanyName and Date\n",
    "merged_df['call_ID'] = merged_df.groupby(['Date','CompanyName']).ngroup()\n",
    "\n",
    "print(merged_df['call_ID'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the merged_df to a csv file\n",
    "merged_df.to_csv('transcripts/post_credit_df.csv', index=False, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df = merged_df[['call_ID', 'presentation', 'QnA', 'CompanyName', 'Ddate']].drop_duplicates().sort_values('call_ID')\n",
    "aggregated_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# export the aggregated_df to a csv file\n",
    "aggregated_df.to_csv('transcripts/post_aggregated_credit_df.csv', index=False, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Data Exploration'''\n",
    "merged_df = pd.read_csv('transcripts/credit_df.csv', delimiter='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the columns are in datetime format\n",
    "merged_df['Date'] = pd.to_datetime(merged_df['Date'])\n",
    "merged_df['Ddate'] = pd.to_datetime(merged_df['Ddate'])\n",
    "merged_df\n",
    "merged_df['t_delta'] = merged_df['Ddate'] - merged_df['Date']\n",
    "\n",
    "# drop all with t_delta > 180\n",
    "merged_df = merged_df[merged_df['t_delta'] <= pd.Timedelta('180 days')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of unique companies\n",
    "print('Unique Companies')\n",
    "print(merged_df['CompanyName'].nunique())\n",
    "\n",
    "# Get the number of unique CUSIPs\n",
    "print('Unique Bonds')\n",
    "print(merged_df['CUSIP'].nunique())\n",
    "\n",
    "# Get the number of unique call_IDs\n",
    "print('Unique Earnings Calls')\n",
    "print(merged_df['call_ID'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurences of each sector\n",
    "# Columns: 'Industrials','Consumer Staples','Financials','Energy','Health Care','Utilities','Information Technology','Real Estate'\n",
    "\n",
    "sector_dict = {\n",
    "    'Industrials': 0,\n",
    "    'Consumer Staples': 0,\n",
    "    'Financials': 0,\n",
    "    'Energy': 0,\n",
    "    'Health Care': 0,\n",
    "    'Utilities': 0,\n",
    "    'Information Technology': 0,\n",
    "    'Real Estate': 0\n",
    "}\n",
    "\n",
    "for sector in sector_dict.keys():\n",
    "    sector_dict[sector] = merged_df[sector].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sector_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total sector count\n",
    "print('Without Sector')\n",
    "sector_dict['None'] = len(merged_df) - sum(sector_dict.values())\n",
    "\n",
    "# create bar plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.bar(sector_dict.keys(), sector_dict.values())\n",
    "plt.title('Sector Distribution')   \n",
    "# make it wider\n",
    "plt.gcf().set_size_inches(16, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "financials = ['CBOE DJIA Volatility Index',\n",
    "    'NASDAQ 100 Index return',\n",
    "    'Manufacturers inventories to sales ratio',\n",
    "    '30 year conventional mortgage rate',\n",
    "    'Communication Services', \n",
    "    'Consumer Discretionary', \n",
    "    'Senior secured',  \n",
    "    'Time to maturity',  \n",
    "    'Equity value',\n",
    "    'CDS availability',\n",
    "    'ActIndustryDistress1',\n",
    "    'ActIndustryDistress2',\n",
    "    'Offering amount',\n",
    "    'Volume',\n",
    "    'Default barrier',\n",
    "    'LTDIssuance2',\n",
    "    'Intangibility',\n",
    "    'Receivables1',\n",
    "    'RR']\n",
    "\n",
    "merged_df['RR'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an graph to show the distibution of RR\n",
    "plt.hist(merged_df['RR'], bins=20)\n",
    "plt.title('Distribution of Recovery Rates')\n",
    "plt.xlabel('Recovery Rate')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiss",
   "language": "python",
   "name": "aiss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
