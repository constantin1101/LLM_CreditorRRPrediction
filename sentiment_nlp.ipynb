{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LassoCV, RidgeCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from presidio_analyzer import AnalyzerEngine, PatternRecognizer, Pattern\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_anonymizer.entities import OperatorConfig\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from source.models import *\n",
    "from source.preprocessing import *\n",
    "from source.variables import *\n",
    "from source.helpers import *\n",
    "\n",
    "from interpret.glassbox import ExplainableBoostingRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = pd.read_csv('transcripts/transcripts.csv', delimiter='|')\n",
    "qna =  pd.read_csv('transcripts/QnA.csv', delimiter='|')\n",
    "\n",
    "# Merge the two dataframes\n",
    "df = pd.merge(transcript, qna[['transcript','filename']], on='filename')\n",
    "\n",
    "# rename transcript_x to presentation and transcript_y to QnA\n",
    "df = df.rename(columns={'transcript_x': 'presentation', 'transcript_y': 'QnA'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RR_CompanyName</th>\n",
       "      <th>Transcript_Mapping</th>\n",
       "      <th>AllNames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-800-FLOWERS.COM, Inc.</td>\n",
       "      <td>1-800-Flowers.com Inc.</td>\n",
       "      <td>1-800-Flowers.com Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3M Company</td>\n",
       "      <td>3M Company</td>\n",
       "      <td>3M Company, 3M Co.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3M Company</td>\n",
       "      <td>3M Co.</td>\n",
       "      <td>3M Company, 3M Co.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A.M. Castle &amp; Co.</td>\n",
       "      <td>A. M. Castle  Co.</td>\n",
       "      <td>A. M. Castle  Co., A.M. Castle  Co., AM Castle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A.M. Castle &amp; Co.</td>\n",
       "      <td>A.M. Castle  Co.</td>\n",
       "      <td>A. M. Castle  Co., A.M. Castle  Co., AM Castle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            RR_CompanyName      Transcript_Mapping  \\\n",
       "0  1-800-FLOWERS.COM, Inc.  1-800-Flowers.com Inc.   \n",
       "1               3M Company              3M Company   \n",
       "2               3M Company                  3M Co.   \n",
       "3        A.M. Castle & Co.       A. M. Castle  Co.   \n",
       "4        A.M. Castle & Co.        A.M. Castle  Co.   \n",
       "\n",
       "                                            AllNames  \n",
       "0                             1-800-Flowers.com Inc.  \n",
       "1                                 3M Company, 3M Co.  \n",
       "2                                 3M Company, 3M Co.  \n",
       "3  A. M. Castle  Co., A.M. Castle  Co., AM Castle...  \n",
       "4  A. M. Castle  Co., A.M. Castle  Co., AM Castle...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = pd.read_csv('data/mapping.csv')\n",
    "\n",
    "# create a new column 'AllNames' that concatenates all versions of 'Company' for a 'CompanyName'\n",
    "mapping['AllNames'] = mapping.groupby('RR_CompanyName')['Transcript_Mapping'].transform(lambda x: ', '.join(x))\n",
    "mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RR</th>\n",
       "      <th>ActIndustryDistress1</th>\n",
       "      <th>ActIndustryDistress2</th>\n",
       "      <th>Senior secured</th>\n",
       "      <th>Senior unsecured</th>\n",
       "      <th>Senior subordinated</th>\n",
       "      <th>Subordinated \\&amp; Junior</th>\n",
       "      <th>Equity value</th>\n",
       "      <th>Default barrier</th>\n",
       "      <th>Net income margin</th>\n",
       "      <th>...</th>\n",
       "      <th>Russell 2000 Price Index return</th>\n",
       "      <th>Russell 2000 Vol 1m</th>\n",
       "      <th>Wilshire US Small-Cap Price Index</th>\n",
       "      <th>Wilshire Small Cap Vol</th>\n",
       "      <th>Ddate</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>CUSIP</th>\n",
       "      <th>LTDIssuance2</th>\n",
       "      <th>Intangibility</th>\n",
       "      <th>Receivables1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.189010</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.28712</td>\n",
       "      <td>0.258205</td>\n",
       "      <td>-0.776257</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01903</td>\n",
       "      <td>21.04</td>\n",
       "      <td>3056.03</td>\n",
       "      <td>808.357714</td>\n",
       "      <td>2004-01-01</td>\n",
       "      <td>Bethlehem Steel Corp.</td>\n",
       "      <td>087509AL9</td>\n",
       "      <td>0.467834468</td>\n",
       "      <td>0.058009127</td>\n",
       "      <td>0.029416454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.553472</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-135.21500</td>\n",
       "      <td>1.269706</td>\n",
       "      <td>-0.564199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01903</td>\n",
       "      <td>21.04</td>\n",
       "      <td>3137.10</td>\n",
       "      <td>974.749210</td>\n",
       "      <td>2004-05-01</td>\n",
       "      <td>T-Mobile US, Inc.</td>\n",
       "      <td>45071TAD7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.200428895</td>\n",
       "      <td>0.032214499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54.315958</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-366.57500</td>\n",
       "      <td>1.081883</td>\n",
       "      <td>-0.671751</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01903</td>\n",
       "      <td>21.05</td>\n",
       "      <td>3178.04</td>\n",
       "      <td>825.987663</td>\n",
       "      <td>2004-01-15</td>\n",
       "      <td>RCN Corporation</td>\n",
       "      <td>749361AC5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005146611</td>\n",
       "      <td>0.032214499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54.798870</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-366.57500</td>\n",
       "      <td>1.081883</td>\n",
       "      <td>-0.671751</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01903</td>\n",
       "      <td>21.05</td>\n",
       "      <td>3178.04</td>\n",
       "      <td>825.987663</td>\n",
       "      <td>2004-01-15</td>\n",
       "      <td>RCN Corporation</td>\n",
       "      <td>749361AD3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005146611</td>\n",
       "      <td>0.029416454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56.666288</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-366.57500</td>\n",
       "      <td>1.081883</td>\n",
       "      <td>-0.671751</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01903</td>\n",
       "      <td>21.05</td>\n",
       "      <td>3178.04</td>\n",
       "      <td>825.987663</td>\n",
       "      <td>2004-01-15</td>\n",
       "      <td>RCN Corporation</td>\n",
       "      <td>749361AG6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005146611</td>\n",
       "      <td>0.029416454</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          RR  ActIndustryDistress1  ActIndustryDistress2  Senior secured  \\\n",
       "0   0.189010                     0                     0               0   \n",
       "1  20.553472                     0                     1               0   \n",
       "2  54.315958                     0                     1               0   \n",
       "3  54.798870                     0                     1               0   \n",
       "4  56.666288                     0                     1               0   \n",
       "\n",
       "   Senior unsecured  Senior subordinated  Subordinated \\& Junior  \\\n",
       "0                 1                    0                       0   \n",
       "1                 1                    0                       0   \n",
       "2                 1                    0                       0   \n",
       "3                 1                    0                       0   \n",
       "4                 1                    0                       0   \n",
       "\n",
       "   Equity value  Default barrier  Net income margin  ...  \\\n",
       "0       1.28712         0.258205          -0.776257  ...   \n",
       "1    -135.21500         1.269706          -0.564199  ...   \n",
       "2    -366.57500         1.081883          -0.671751  ...   \n",
       "3    -366.57500         1.081883          -0.671751  ...   \n",
       "4    -366.57500         1.081883          -0.671751  ...   \n",
       "\n",
       "   Russell 2000 Price Index return  Russell 2000 Vol 1m  \\\n",
       "0                          0.01903                21.04   \n",
       "1                          0.01903                21.04   \n",
       "2                          0.01903                21.05   \n",
       "3                          0.01903                21.05   \n",
       "4                          0.01903                21.05   \n",
       "\n",
       "   Wilshire US Small-Cap Price Index  Wilshire Small Cap Vol      Ddate  \\\n",
       "0                            3056.03              808.357714 2004-01-01   \n",
       "1                            3137.10              974.749210 2004-05-01   \n",
       "2                            3178.04              825.987663 2004-01-15   \n",
       "3                            3178.04              825.987663 2004-01-15   \n",
       "4                            3178.04              825.987663 2004-01-15   \n",
       "\n",
       "             CompanyName      CUSIP  LTDIssuance2  Intangibility  Receivables1  \n",
       "0  Bethlehem Steel Corp.  087509AL9   0.467834468    0.058009127   0.029416454  \n",
       "1      T-Mobile US, Inc.  45071TAD7             0    0.200428895   0.032214499  \n",
       "2        RCN Corporation  749361AC5             0    0.005146611   0.032214499  \n",
       "3        RCN Corporation  749361AD3             0    0.005146611   0.029416454  \n",
       "4        RCN Corporation  749361AG6             0    0.005146611   0.029416454  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load recovery rates\n",
    "rr = pd.read_csv('data/RR_Bonds.csv')\n",
    "rr = rr[['Ddate', 'RR', 'CompanyName', 'CUSIP', 'LTDIssuance2', 'Intangibility', 'Receivables1']]\n",
    "\n",
    "preprocessed_df = pd.read_csv('data/preprocessed_bond_data.csv')\n",
    "\n",
    "# Add rr columns to preprocessed_df on index\n",
    "preprocessed_df['RR'] = rr['RR']\n",
    "preprocessed_df['Ddate'] = rr['Ddate']\n",
    "preprocessed_df['CompanyName'] = rr['CompanyName']\n",
    "preprocessed_df['CUSIP'] = rr['CUSIP']\n",
    "preprocessed_df['LTDIssuance2'] = rr['LTDIssuance2']\n",
    "preprocessed_df['Intangibility'] = rr['Intangibility']\n",
    "preprocessed_df['Receivables1'] = rr['Receivables1']\n",
    "\n",
    "rr = preprocessed_df\n",
    "\n",
    "# Convert 'Date' column to datetime\n",
    "rr['Ddate'] = pd.to_datetime(rr['Ddate'], errors='coerce')\n",
    "rr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge rr with mapping on CompanyName and RR_CompanyName\n",
    "rr = rr.merge(mapping, left_on='CompanyName', right_on='RR_CompanyName')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ally Financial Inc.               10317\n",
      "CIT Group Inc.                    10185\n",
      "Lehman Brothers Holdings, Inc.     2853\n",
      "Charter Communications, Inc.       2144\n",
      "Sempra Energy                      1147\n",
      "                                  ...  \n",
      "Frontier Group Holdings, Inc.         1\n",
      "Dayton Superior Corporation           1\n",
      "Franklin Bank Corp.                   1\n",
      "Kellwood Company, LLC                 1\n",
      "Turning Point Brands, Inc.            1\n",
      "Name: CompanyName, Length: 210, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# join with df on Company and Transcripts_Mapping\n",
    "merged_df = rr.merge(df, left_on='Transcript_Mapping', right_on='Company')\n",
    "print(merged_df['CompanyName'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lehman Brothers Holdings, Inc.      317\n",
      "CIT Group Inc.                      291\n",
      "Charter Communications, Inc.         28\n",
      "Ford Motor Company                   19\n",
      "iStar Inc.                           17\n",
      "                                   ... \n",
      "Centrus Energy Corp.                  1\n",
      "Education Management Corporation      1\n",
      "Venoco, Inc.                          1\n",
      "Exelon Corporation                    1\n",
      "Kellwood Company, LLC                 1\n",
      "Name: CompanyName, Length: 159, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Ensure the columns are in datetime format\n",
    "merged_df['Date'] = pd.to_datetime(merged_df['Date'])\n",
    "merged_df['Ddate'] = pd.to_datetime(merged_df['Ddate'])\n",
    "\n",
    "# Compute the difference in days\n",
    "merged_df['diff'] = (merged_df['Ddate'] - merged_df['Date']).dt.days\n",
    "\n",
    "merged_df = merged_df[merged_df['Ddate']>merged_df['Date']]\n",
    "merged_df = merged_df.sort_values(by='Date').groupby(['CUSIP']).tail(1)\n",
    "\n",
    "print(merged_df['CompanyName'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Keywords for each credit factor\n",
    "credit_keywords = {\n",
    "    'Profitability': ['revenue', 'cost', 'profit', 'earnings', 'margins', 'performance', 'income', 'loss', 'decline', 'decrease', 'outlook', 'guidance'],\n",
    "    'Liquidity': ['cash', 'liquidity', 'credit', 'flow', 'operations', 'expenditures', 'free cash', 'working capital', 'insolvency', 'crunch', 'flexibility', 'funding'],\n",
    "    'Leverage': ['debt', 'leverage', 'refinancing', 'reduction', 'interest', 'coverage', 'repayments', 'compliance', 'rating', 'default', 'restructuring'],\n",
    "    'Operating': ['sales', 'market share', 'efficiency', 'cost', 'position', 'conditions', 'production', 'challenges', 'decline', 'improvement'],\n",
    "    'Market': ['stock', 'market', 'investor', 'volatility', 'shareholder', 'confidence', 'buybacks', 'dilution', 'perception'],\n",
    "    'Management': ['management', 'strategic', 'restructuring', 'strategy', 'adaptability', 'leadership', 'initiatives', 'governance', 'organizational', 'CEO', 'board']\n",
    "}\n",
    "\n",
    "# Function to identify sections with potential bankruptcy indicators\n",
    "def identify_bankruptcy_indicators(transcript, keywords):\n",
    "    sentences = nltk.sent_tokenize(transcript)\n",
    "    indicator_sentences = []\n",
    "    for sentence in sentences:\n",
    "        for key in keywords:\n",
    "            if any(re.search(r'\\b' + re.escape(word) + r'\\b', sentence, re.IGNORECASE) for word in keywords[key]):\n",
    "                indicator_sentences.append(sentence)\n",
    "                break\n",
    "\n",
    "    return ' '.join(indicator_sentences)\n",
    "\n",
    "# Function to clean text by stemming and replacing numbers with magnitude tokens\n",
    "def clean_text(text):\n",
    "    # Replace numbers with tokens\n",
    "    text = re.sub(r'\\b\\d+(\\.\\d+)?\\s?(billion|bln)\\b', 'bln', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b\\d+(\\.\\d+)?\\s?(million|mln)\\b', 'mln', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b\\d{1,3}(,\\d{3})*(\\.\\d+)?\\b', 'num', text)  # Replace remaining numbers with 'num'\n",
    "    \n",
    "    # Remove non-alphanumeric characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize, stem, and rejoin\n",
    "    words = nltk.word_tokenize(text)\n",
    "    stemmed_words = [stemmer.stem(word) for word in words if len(word) > 1]  # Remove single characters that might be noise\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "# Transform transcript to lowercase\n",
    "merged_df['presentation'] = merged_df['presentation'].str.lower()\n",
    "merged_df['QnA'] = merged_df['QnA'].str.lower()\n",
    "\n",
    "# Apply function to identify bankruptcy indicators\n",
    "merged_df['presentation_cleaned'] = merged_df['presentation'].apply(lambda x: identify_bankruptcy_indicators(x, credit_keywords))\n",
    "merged_df['QnA_cleaned'] = merged_df['QnA'].apply(lambda x: identify_bankruptcy_indicators(x, credit_keywords))\n",
    "\n",
    "# Apply function to clean text\n",
    "merged_df['presentation'] = merged_df['presentation'].apply(clean_text)\n",
    "merged_df['QnA'] = merged_df['QnA'].apply(clean_text)\n",
    "\n",
    "merged_df['presentation_cleaned'] = merged_df['presentation_cleaned'].apply(clean_text)\n",
    "merged_df['QnA_cleaned'] = merged_df['QnA_cleaned'].apply(clean_text)\n",
    "\n",
    "# reset index\n",
    "merged_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Uncertainty</th>\n",
       "      <th>Litigious</th>\n",
       "      <th>StrongModal</th>\n",
       "      <th>WeakModal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ABANDON</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ABANDONED</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ABANDONING</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ABANDONMENT</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ABANDONMENTS</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word  Negative  Positive  Uncertainty  Litigious  StrongModal  \\\n",
       "9        ABANDON         1         0            0          0            0   \n",
       "10     ABANDONED         1         0            0          0            0   \n",
       "11    ABANDONING         1         0            0          0            0   \n",
       "12   ABANDONMENT         1         0            0          0            0   \n",
       "13  ABANDONMENTS         1         0            0          0            0   \n",
       "\n",
       "    WeakModal  \n",
       "9           0  \n",
       "10          0  \n",
       "11          0  \n",
       "12          0  \n",
       "13          0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment = pd.read_csv('data/sentiment.csv')\n",
    "\n",
    "# drop all columns except Word, Negative, Positive, Uncertainty, Litigious, Strong_Modal, Weak_Modal\n",
    "sentiment = sentiment[['Word', 'Negative', 'Positive', 'Uncertainty', 'Litigious', 'StrongModal', 'WeakModal']]\n",
    "\n",
    "# change any number in Negative, Positive, Uncertainty, Litigious, Strong_Modal, Weak_Modal to 1\n",
    "sentiment.loc[sentiment['Negative'] != 0, 'Negative'] = 1\n",
    "sentiment.loc[sentiment['Positive'] != 0, 'Positive'] = 1\n",
    "sentiment.loc[sentiment['Uncertainty'] != 0, 'Uncertainty'] = 1\n",
    "sentiment.loc[sentiment['Litigious'] != 0, 'Litigious'] = 1\n",
    "sentiment.loc[sentiment['StrongModal'] != 0, 'StrongModal'] = 1\n",
    "sentiment.loc[sentiment['WeakModal'] != 0, 'WeakModal'] = 1\n",
    "\n",
    "# drop all rows where all columns are 0\n",
    "sentiment = sentiment[(sentiment[['Negative', 'Positive', 'Uncertainty', 'Litigious', 'StrongModal', 'WeakModal']] != 0).any(axis=1)]\n",
    "\n",
    "sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make all words in Word column lowercase and stem them\n",
    "sentiment['Word'] = sentiment['Word'].str.lower()\n",
    "sentiment['Word'] = sentiment['Word'].apply(lambda x: stemmer.stem(x))\n",
    "\n",
    "# drop duplicates\n",
    "sentiment.drop_duplicates(subset='Word', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in lower case\n",
    "financial_lexicon = sentiment['Word'].tolist()\n",
    "\n",
    "def filter_tokens_by_lexicon(text, lexicon):\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [word for word in tokens if word in lexicon]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "filtered_presentation = [filter_tokens_by_lexicon(transcript, financial_lexicon) for transcript in merged_df['presentation']]\n",
    "filtered_presentation_cleaned = [filter_tokens_by_lexicon(transcript, financial_lexicon) for transcript in merged_df['presentation_cleaned']]\n",
    "filtered_qna = [filter_tokens_by_lexicon(transcript, financial_lexicon) for transcript in merged_df['QnA']]\n",
    "filtered_qna_cleaned = [filter_tokens_by_lexicon(transcript, financial_lexicon) for transcript in merged_df['QnA_cleaned']]\n",
    "\n",
    "# Build the DTM\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "presentation_dtm = vectorizer.fit_transform(filtered_presentation)\n",
    "presentation_dtm_df = pd.DataFrame(presentation_dtm.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "presentation_cleaned_dtm = vectorizer.fit_transform(filtered_presentation_cleaned)\n",
    "presentation_cleaned_dtm_df = pd.DataFrame(presentation_cleaned_dtm.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "qna_dtm = vectorizer.fit_transform(filtered_qna)\n",
    "qna_dtm_df = pd.DataFrame(qna_dtm.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "qna_cleaned_dtm = vectorizer.fit_transform(filtered_qna_cleaned)\n",
    "qna_cleaned_dtm_df = pd.DataFrame(qna_cleaned_dtm.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of the dataframes\n",
    "dtm_dict = {\n",
    "    'presentation': presentation_dtm_df,\n",
    "    'presentation_cleaned': presentation_cleaned_dtm_df,\n",
    "    'QnA': qna_dtm_df,\n",
    "    'QnA_cleaned': qna_cleaned_dtm_df\n",
    "}\n",
    "\n",
    "# create a dictionary of dataframes for the 4 types which based on merged_df['RR']\n",
    "dict_rr = {\n",
    "    'presentation': merged_df[['RR','Date', 'presentation']].reset_index(drop=True),\n",
    "    'presentation_cleaned': merged_df[['RR','Date', 'presentation_cleaned']].reset_index(drop=True),\n",
    "    'QnA': merged_df[['RR','Date', 'QnA']].reset_index(drop=True),\n",
    "    'QnA_cleaned': merged_df[['RR','Date', 'QnA_cleaned']].reset_index(drop=True),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lists of categories\n",
    "negative_words = sentiment[sentiment['Negative'] == 1]['Word'].str.lower().tolist()\n",
    "positive_words = sentiment[sentiment['Positive'] == 1]['Word'].str.lower().tolist()\n",
    "uncertainty_words = sentiment[sentiment['Uncertainty'] == 1]['Word'].str.lower().tolist()\n",
    "litigious_words = sentiment[sentiment['Litigious'] == 1]['Word'].str.lower().tolist()\n",
    "strong_modal_words = sentiment[sentiment['StrongModal'] == 1]['Word'].str.lower().tolist()\n",
    "weak_modal_words = sentiment[sentiment['WeakModal'] == 1]['Word'].str.lower().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dtm_dict:\n",
    "\n",
    "    # add sentiment scores to the dataframe\n",
    "    dict_rr[key]['NegativeScore'] = 0\n",
    "    dict_rr[key]['PositiveScore'] = 0\n",
    "    dict_rr[key]['UncertaintyScore'] = 0\n",
    "    dict_rr[key]['LitigiousScore'] = 0\n",
    "    dict_rr[key]['StrongModalScore'] = 0\n",
    "    dict_rr[key]['WeakModalScore'] = 0\n",
    "\n",
    "    # iterate of columns in the DTM and add sentiment scores to the dataframe based on categories\n",
    "    for column in dtm_dict[key].columns:\n",
    "        if column in negative_words:\n",
    "            dict_rr[key]['NegativeScore'] += dtm_dict[key][column]\n",
    "        if column in positive_words:\n",
    "            dict_rr[key]['PositiveScore'] += dtm_dict[key][column]\n",
    "        if column in uncertainty_words:\n",
    "            dict_rr[key]['UncertaintyScore'] += dtm_dict[key][column]\n",
    "        if column in litigious_words:\n",
    "            dict_rr[key]['LitigiousScore'] += dtm_dict[key][column]\n",
    "        if column in strong_modal_words:\n",
    "            dict_rr[key]['StrongModalScore'] += dtm_dict[key][column]\n",
    "        if column in weak_modal_words:\n",
    "            dict_rr[key]['WeakModalScore'] += dtm_dict[key][column]\n",
    "    \n",
    "    # compute sentiment = PositiveScore - NegativeScore / Total word count\n",
    "    dict_rr[key]['Sentiment'] = (dict_rr[key]['PositiveScore'] - dict_rr[key]['NegativeScore']) / len(dict_rr[key][key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dict_rr:\n",
    "    # transform RR to label 1 if RR > 50 else 0\n",
    "    dict_rr[key]['label'] = dict_rr[key]['RR'].apply(lambda x: 1 if x > 50 else 0)\n",
    "\n",
    "    # if label == 1 and Sentiment > 0 then TP\n",
    "    dict_rr[key]['Prediction'] = dict_rr[key].apply(lambda x: 1 if x['label'] == 1 and x['Sentiment'] > 0 else 0, axis=1)\n",
    "    # if label == 0 and Sentiment < 0 then TN\n",
    "    dict_rr[key]['Prediction'] = dict_rr[key].apply(lambda x: 1 if x['label'] == 0 and x['Sentiment'] < 0 else 0, axis=1)\n",
    "    # if label == 1 and Sentiment < 0 then FN\n",
    "    dict_rr[key]['Prediction'] = dict_rr[key].apply(lambda x: 1 if x['label'] == 1 and x['Sentiment'] < 0 else 0, axis=1)\n",
    "    # if label == 0 and Sentiment > 0 then FP\n",
    "    dict_rr[key]['Prediction'] = dict_rr[key].apply(lambda x: 1 if x['label'] == 0 and x['Sentiment'] > 0 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for presentation: 0.1572093023255814\n",
      "Accuracy for presentation_cleaned: 0.11906976744186047\n",
      "Accuracy for QnA: 0.03162790697674418\n",
      "Accuracy for QnA_cleaned: 0.04279069767441861\n"
     ]
    }
   ],
   "source": [
    "# Compute accuracy for each type\n",
    "for key in dict_rr:\n",
    "    accuracy = dict_rr[key]['Prediction'].sum() / len(dict_rr[key])\n",
    "    print(f'Accuracy for {key}: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiss",
   "language": "python",
   "name": "aiss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
