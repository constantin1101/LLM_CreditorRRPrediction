{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI, RateLimitError\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "import time\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from source.models import *\n",
    "from source.preprocessing import *\n",
    "from source.variables import *\n",
    "from source.helpers import *\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = pd.read_csv('transcripts/transcripts.csv', delimiter='|')\n",
    "qna =  pd.read_csv('transcripts/QnA.csv', delimiter='|')\n",
    "\n",
    "# Merge the two dataframes\n",
    "df = pd.merge(transcript, qna[['transcript','filename']], on='filename')\n",
    "\n",
    "# rename transcript_x to presentation and transcript_y to QnA\n",
    "df = df.rename(columns={'transcript_x': 'presentation', 'transcript_y': 'QnA'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = pd.read_csv('data/mapping.csv')\n",
    "\n",
    "# create a new column 'AllNames' that concatenates all versions of 'Company' for a 'CompanyName'\n",
    "mapping['AllNames'] = mapping.groupby('RR_CompanyName')['Transcript_Mapping'].transform(lambda x: ', '.join(x))\n",
    "mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load recovery rates\n",
    "rr = pd.read_csv('data/RR_Bonds.csv')\n",
    "rr = rr[['Ddate', 'RR', 'CompanyName', 'CUSIP', 'LTDIssuance2', 'Intangibility', 'Receivables1']]\n",
    "\n",
    "preprocessed_df = pd.read_csv('data/preprocessed_bond_data.csv')\n",
    "\n",
    "# Add rr columns to preprocessed_df on index\n",
    "preprocessed_df['RR'] = rr['RR']\n",
    "preprocessed_df['Ddate'] = rr['Ddate']\n",
    "preprocessed_df['CompanyName'] = rr['CompanyName']\n",
    "preprocessed_df['CUSIP'] = rr['CUSIP']\n",
    "preprocessed_df['LTDIssuance2'] = rr['LTDIssuance2']\n",
    "preprocessed_df['Intangibility'] = rr['Intangibility']\n",
    "preprocessed_df['Receivables1'] = rr['Receivables1']\n",
    "\n",
    "rr = preprocessed_df\n",
    "\n",
    "# Convert 'Date' column to datetime\n",
    "rr['Ddate'] = pd.to_datetime(rr['Ddate'], errors='coerce')\n",
    "rr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge rr with mapping on CompanyName and RR_CompanyName\n",
    "rr = rr.merge(mapping, left_on='CompanyName', right_on='RR_CompanyName')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join with df on Company and Transcripts_Mapping\n",
    "merged_df = rr.merge(df, left_on='Transcript_Mapping', right_on='Company')\n",
    "print(merged_df['CompanyName'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the columns are in datetime format\n",
    "merged_df['Date'] = pd.to_datetime(merged_df['Date'])\n",
    "merged_df['Ddate'] = pd.to_datetime(merged_df['Ddate'])\n",
    "\n",
    "# Compute the difference in days\n",
    "merged_df['diff'] = (merged_df['Ddate'] - merged_df['Date']).dt.days\n",
    "\n",
    "merged_df = merged_df[merged_df['Ddate']>merged_df['Date']]\n",
    "merged_df = merged_df.sort_values(by='Date').groupby(['CUSIP']).tail(1)\n",
    "\n",
    "print(merged_df['CompanyName'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Keywords for each credit factor\n",
    "credit_keywords = {\n",
    "    'Profitability': ['revenue', 'cost', 'profit', 'earnings', 'margins', 'performance', 'income', 'loss', 'decline', 'decrease', 'outlook', 'guidance'],\n",
    "    'Liquidity': ['cash', 'liquidity', 'credit', 'flow', 'operations', 'expenditures', 'free cash', 'working capital', 'insolvency', 'crunch', 'flexibility', 'funding'],\n",
    "    'Leverage': ['debt', 'leverage', 'refinancing', 'reduction', 'interest', 'coverage', 'repayments', 'compliance', 'rating', 'default', 'restructuring'],\n",
    "    'Operating': ['sales', 'market share', 'efficiency', 'cost', 'position', 'conditions', 'production', 'challenges', 'decline', 'improvement'],\n",
    "    'Market': ['stock', 'market', 'investor', 'volatility', 'shareholder', 'confidence', 'buybacks', 'dilution', 'perception'],\n",
    "    'Management': ['management', 'strategic', 'restructuring', 'strategy', 'adaptability', 'leadership', 'initiatives', 'governance', 'organizational', 'CEO', 'board']\n",
    "}\n",
    "\n",
    "# Function to identify sections with potential bankruptcy indicators\n",
    "def identify_bankruptcy_indicators(transcript, keywords):\n",
    "    sentences = nltk.sent_tokenize(transcript)\n",
    "    indicator_sentences = []\n",
    "    for sentence in sentences:\n",
    "        for key in keywords:\n",
    "            if any(re.search(r'\\b' + re.escape(word) + r'\\b', sentence, re.IGNORECASE) for word in keywords[key]):\n",
    "                indicator_sentences.append(sentence)\n",
    "                break\n",
    "\n",
    "    return ' '.join(indicator_sentences)\n",
    "\n",
    "# Function to clean text by stemming and replacing numbers with magnitude tokens\n",
    "def clean_text(text):\n",
    "    # Replace numbers with tokens\n",
    "    text = re.sub(r'\\b\\d+(\\.\\d+)?\\s?(billion|bln)\\b', 'bln', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b\\d+(\\.\\d+)?\\s?(million|mln)\\b', 'mln', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b\\d{1,3}(,\\d{3})*(\\.\\d+)?\\b', 'num', text)  # Replace remaining numbers with 'num'\n",
    "    \n",
    "    # Remove non-alphanumeric characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize, stem, and rejoin\n",
    "    words = nltk.word_tokenize(text)\n",
    "    stemmed_words = [stemmer.stem(word) for word in words if len(word) > 1]  # Remove single characters that might be noise\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "# Transform transcript to lowercase\n",
    "merged_df['presentation'] = merged_df['presentation'].str.lower()\n",
    "merged_df['QnA'] = merged_df['QnA'].str.lower()\n",
    "\n",
    "# Apply function to identify bankruptcy indicators\n",
    "merged_df['presentation'] = merged_df['presentation'].apply(lambda x: identify_bankruptcy_indicators(x, credit_keywords))\n",
    "merged_df['QnA'] = merged_df['QnA'].apply(lambda x: identify_bankruptcy_indicators(x, credit_keywords))\n",
    "\n",
    "# Apply function to clean text\n",
    "merged_df['presentation'] = merged_df['presentation'].apply(clean_text)\n",
    "merged_df['QnA'] = merged_df['QnA'].apply(clean_text)\n",
    "\n",
    "# reset index\n",
    "merged_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count unique transcripts in transcripts['Cleaned_Bankruptcy_Indicators']\n",
    "print(merged_df['presentation'].nunique())\n",
    "print(merged_df['QnA'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a number to each transcript based on the 196 unique transcripts\n",
    "merged_df.reset_index(drop=True, inplace=True)\n",
    "merged_df['transcript_number'] = merged_df['presentation'].factorize()[0]\n",
    "\n",
    "number_transcript = merged_df[['transcript_number', 'presentation', 'QnA']].drop_duplicates().sort_values('transcript_number')\n",
    "\n",
    "number_transcript.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count tokens in merged_df['presentation']\n",
    "print(number_transcript['presentation'].apply(lambda x: len(x.split())).sum())\n",
    "print(number_transcript['QnA'].apply(lambda x: len(x.split())).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export number_transcript to csv\n",
    "#number_transcript.to_csv('transcripts/embeddings_input.csv', index=False)\n",
    "number_transcript = pd.read_csv('transcripts/embeddings_input.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide number_transcript['presentation'] into 6 parts\n",
    "presentation_list = number_transcript['presentation'].to_list()\n",
    "QnA_list = number_transcript['QnA'].to_list()\n",
    "\n",
    "start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "#all_embeddings = []\n",
    "counter = 0\n",
    "for i in presentation_list:\n",
    "    if counter == start:\n",
    "        try:\n",
    "            embeddings = client.embeddings.create(\n",
    "            model=\"text-embedding-3-large\",\n",
    "            input=i,\n",
    "            encoding_format=\"float\"\n",
    "            )\n",
    "            all_embeddings.append(embeddings)\n",
    "        except RateLimitError as e:\n",
    "            print(f\"Rate limit exceeded: {e}\")\n",
    "            time.sleep(60)\n",
    "    else:\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = len(all_embeddings)\n",
    "print(f\"Start: {start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_transcript['presentation_embeddings'] = [None] * len(number_transcript)\n",
    "number_transcript.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# extract embeddings from all_embeddings and add to number_transcript as 'presentation_embeddings'\n",
    "for i in range(0, 196):\n",
    "    print(all_embeddings[i].data[0].embedding)\n",
    "    number_transcript['presentation_embeddings'][i] = all_embeddings[i].data[0].embedding\n",
    "\n",
    "number_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge output_df with transcripts on 'transcript_number' and index\n",
    "merged_df = pd.merge(merged_df,\n",
    "                          number_transcript[['transcript_number', 'presentation_embeddings']],\n",
    "                          on='transcript_number',\n",
    "                          how='left')\n",
    "\n",
    "checkpoint = merged_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint.to_csv('transcripts/LLM_embeddings_presentation.csv', index=False)\n",
    "#checkpoint = pd.read_csv('transcripts/LLM_embeddings_presentation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of selected supporting features\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "supporting_features_1 = [\n",
    "    #'CBOE DJIA Volatility Index',\n",
    "    #'NASDAQ 100 Index return',\n",
    "    #'Manufacturers inventories to sales ratio',\n",
    "    #'30 year conventional mortgage rate',\n",
    "    #'Communication Services', \n",
    "    #'Consumer Discretionary', \n",
    "    #'Senior secured',  \n",
    "    #'Time to maturity',  \n",
    "    #'Equity value',\n",
    "    #'CDS availability',\n",
    "    'ActIndustryDistress1',\n",
    "    'ActIndustryDistress2',\n",
    "    'Offering amount',\n",
    "    'Volume',\n",
    "    'Industrials','Consumer Staples','Financials','Energy','Health Care','Utilities','Information Technology','Real Estate'\n",
    "]\n",
    "\n",
    "supporting_features_2 = [\n",
    "    'Default barrier',\n",
    "    'LTDIssuance2',\n",
    "    'Intangibility',\n",
    "    'Receivables1',\n",
    "]\n",
    "\n",
    "embeddings_columns = ['presentation_embeddings']\n",
    "\n",
    "# Select the supporting features, nlp_lables, and RR from final_df\n",
    "final_df = checkpoint[['Date'] \n",
    "                    #+ supporting_features_1\n",
    "                    #+ supporting_features_2\n",
    "                    + embeddings_columns \n",
    "                    + ['RR']]\n",
    "\n",
    "# replace #DIV/0! & Nan with 0\n",
    "# Replace '#DIV/0!' with NaN\n",
    "final_df.replace('#DIV/0!', np.nan, inplace=True)\n",
    "final_df = final_df.fillna(0)\n",
    "\n",
    "# make sure all values are numeric except for the Date column\n",
    "final_df = final_df.apply(pd.to_numeric, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform final_df['presentation_embeddings'] to columns\n",
    "final_df = pd.concat([final_df, final_df['presentation_embeddings'].apply(pd.Series)], axis=1)\n",
    "final_df.drop('presentation_embeddings', axis=1, inplace=True)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In-Sample-Regression\n",
    "y_train, y_test = final_df['RR'], final_df['RR']\n",
    "X_train, X_test = final_df.drop(columns=['RR', 'Date']), final_df.drop(columns=['RR', 'Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out of sample regression\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(0.8 * len(final_df))\n",
    "X_train, X_test = final_df.drop(columns=['RR', 'Date']).iloc[:train_size], final_df.drop(columns=['RR', 'Date']).iloc[train_size:]\n",
    "y_train, y_test = final_df['RR'].iloc[:train_size], final_df['RR'].iloc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bond data 1 + LLM features\n",
    "\n",
    "X_train = sm.add_constant(X_train)\n",
    "X_test = sm.add_constant(X_test)\n",
    "\n",
    "# Fit the model\n",
    "model = sm.OLS(y_train, X_train).fit()\n",
    "\n",
    "# Print the summary of the model which includes p-values and significance levels\n",
    "print(model.summary())\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Compute and print evaluation metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Root Mean Squared Error: {np.sqrt(mse)}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "# other metric\n",
    "# Calculate the residuals\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# Plot the residuals\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(y_test, residuals)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Recovery Rate')\n",
    "plt.ylabel('Residuals')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply ridge regression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "y_pred_ridge = ridge_model.predict(X_test)\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "print(f'R^2 Score for Ridge Regression on {r2_ridge}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_sample = True\n",
    "\n",
    "if in_sample:\n",
    "    # In-Sample-Regression\n",
    "    y_train, y_test = final_df['RR'], final_df['RR']\n",
    "    X_train, X_test = final_df.drop(columns=['RR', 'Date']), final_df.drop(columns=['RR', 'Date'])\n",
    "\n",
    "else:\n",
    "    # out of sample regression\n",
    "    # Split the data into training and testing sets\n",
    "    train_size = int(0.7 * len(final_df))\n",
    "    X_train, X_test = final_df.drop(columns=['RR', 'Date']).iloc[:train_size], final_df.drop(columns=['RR', 'Date']).iloc[train_size:]\n",
    "    y_train, y_test = final_df['RR'].iloc[:train_size], final_df['RR'].iloc[train_size:]\n",
    "\n",
    "# change y to binary based on >< 50\n",
    "#y_train = np.where(y_train > 50, 0, 1)\n",
    "#y_test = np.where(y_test > 50, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train Model 1: Sentiment Prediction using Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_sentiment_model(X_train, y_train):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    log_reg = LogisticRegression()\n",
    "    log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "    return log_reg, scaler\n",
    "\n",
    "# Train the sentiment model\n",
    "sentiment_model, scaler_sentiment = train_sentiment_model(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "X_test_scaled = scaler_sentiment.transform(X_test)\n",
    "y_pred_sentiment = sentiment_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the accuracy of the sentiment model\n",
    "accuracy = accuracy_score(y_test, y_pred_sentiment)\n",
    "print(f'Sentiment Model Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train Model 2: Recovery Rate Prediction using Ridge Regression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "bonds_df = final_df.copy()\n",
    "\n",
    "\n",
    "def train_recovery_rate_model(X_train, y_train):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    ridge_reg = Ridge(alpha=1.0)\n",
    "    ridge_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "    return ridge_reg, scaler\n",
    "\n",
    "# Splitting the data for training and testing\n",
    "X_train_bonds, X_test_bonds, y_train_bonds, y_test_bonds = train_test_split(X_train, y_train, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the recovery rate model\n",
    "recovery_rate_model, scaler_recovery = train_recovery_rate_model(X_train_bonds, y_train_bonds)\n",
    "\n",
    "# Predict on test data\n",
    "X_test_bonds_scaled = scaler_recovery.transform(X_test_bonds)\n",
    "y_pred_recovery = recovery_rate_model.predict(X_test_bonds_scaled)\n",
    "\n",
    "# Evaluate the performance of the recovery rate model\n",
    "mse = mean_squared_error(y_test_bonds, y_pred_recovery)\n",
    "r2 = r2_score(y_test_bonds, y_pred_recovery)\n",
    "print(f'Recovery Rate Model MSE: {mse:.4f}')\n",
    "print(f'Recovery Rate Model R-squared: {r2:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiss",
   "language": "python",
   "name": "aiss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
